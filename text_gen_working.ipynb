{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_gen_working.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1dNwFNjty4I"
      },
      "source": [
        "Generiamo del testo dando in input un dataset txt \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmRBq1vZuIeL",
        "outputId": "a4766e85-d0a2-45f9-dce6-5271de34d7ae"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install numpy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDmpH_W1ty4L"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq_w_zpMty4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db00a227-0810-40e5-cb65-dfd08a278715"
      },
      "source": [
        "#path_to_file = tf.keras.utils.get_file(\"digital.txt\", \"https://raw.githubusercontent.com/carloocchiena/rnn_text_generation/main/dataset/digitransf.txt\" )\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file(\"metal.txt\", \"https://raw.githubusercontent.com/carloocchiena/rnn_text_generation/main/dataset/metal.txt\" )\n",
        "\n",
        "#when refreshing the dataset, clean also the keras temp folder C:\\Users\\Carlo\\.keras\\datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/carloocchiena/rnn_text_generation/main/dataset/metal.txt\n",
            "221184/219236 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZH5NRvIty4N",
        "outputId": "395af16c-5580-4112-c74c-e13344461712"
      },
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Lenght of text: {len(text)} characthers\")\n",
        "print (text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lenght of text: 219101 characthers\n",
            "1. Hit The Lights\r\n",
            "\r\n",
            "[James Hetfield]\r\n",
            "[Lars Ulrich]\r\n",
            "\r\n",
            "No life till leather\r\n",
            "We are gonna kick some ass tonight\r\n",
            "We got the metal madness\r\n",
            "When our fans start screaming\r\n",
            "It's right well alright\r\n",
            "When we start to rock\r\n",
            "We never want to stop again\r\n",
            "\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmlbPn5Ety4O",
        "outputId": "4490c2dc-0e01-4d9a-8246-9dd8609ad906"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print (f\"{len(vocab)} unique characters\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWHwEKNUty4O"
      },
      "source": [
        "example_texts =[\"abcdefghijklmnopqrstuvw\", \"xyz\"]\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_U-xQimty4P"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQmbhrJty4P"
      },
      "source": [
        "ids = ids_from_chars(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20VtGyioty4P"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roZzBkdWty4Q"
      },
      "source": [
        "chars  = chars_from_ids(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG7TsrAhty4Q",
        "outputId": "f0a5beaa-b2c1-46d9-ebc2-a92a2141deb7"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefghijklmnopqrstuvw', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyTzrY5_ty4Q"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TimFgoLBty4R",
        "outputId": "133e9105-743e-463d-accb-6c7f0a0f1270"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(219101,), dtype=int64, numpy=array([17, 14,  3, ..., 72,  2,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhr0pCioty4R"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lsqhqOVty4S",
        "outputId": "43a7a24b-9729-4774-f58e-c444edc75e17"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            ".\n",
            " \n",
            "H\n",
            "i\n",
            "t\n",
            " \n",
            "T\n",
            "h\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQ28Bu0ty4S"
      },
      "source": [
        "seq_length = 100\n",
        "example_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44D8SI9ty4S",
        "outputId": "8c366510-0fb9-4967-fd61-d6cdbd13e461"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'1' b'.' b' ' b'H' b'i' b't' b' ' b'T' b'h' b'e' b' ' b'L' b'i' b'g'\n",
            " b'h' b't' b's' b'\\r' b'\\n' b'\\r' b'\\n' b'[' b'J' b'a' b'm' b'e' b's' b' '\n",
            " b'H' b'e' b't' b'f' b'i' b'e' b'l' b'd' b']' b'\\r' b'\\n' b'[' b'L' b'a'\n",
            " b'r' b's' b' ' b'U' b'l' b'r' b'i' b'c' b'h' b']' b'\\r' b'\\n' b'\\r' b'\\n'\n",
            " b'N' b'o' b' ' b'l' b'i' b'f' b'e' b' ' b't' b'i' b'l' b'l' b' ' b'l'\n",
            " b'e' b'a' b't' b'h' b'e' b'r' b'\\r' b'\\n' b'W' b'e' b' ' b'a' b'r' b'e'\n",
            " b' ' b'g' b'o' b'n' b'n' b'a' b' ' b'k' b'i' b'c' b'k' b' ' b's' b'o'\n",
            " b'm' b'e' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vReeSP6zty4T",
        "outputId": "115fb99b-e29a-454b-ff58-0c1207685781"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'1. Hit The Lights\\r\\n\\r\\n[James Hetfield]\\r\\n[Lars Ulrich]\\r\\n\\r\\nNo life till leather\\r\\nWe are gonna kick some '\n",
            "b\"ass tonight\\r\\nWe got the metal madness\\r\\nWhen our fans start screaming\\r\\nIt's right well alright\\r\\nWhen w\"\n",
            "b'e start to rock\\r\\nWe never want to stop again\\r\\n\\r\\nHit the lights\\r\\nHit the lights\\r\\nHit the lights\\r\\n\\r\\nYou'\n",
            "b' know our fans are insane\\r\\nWe are gonna blow this place away\\r\\nwith volume higher\\r\\nThan anything today'\n",
            "b' the only way\\r\\nWhen we start to rock\\r\\nWe never want to stop again\\r\\n\\r\\nHit the lights\\r\\nHit the lights\\r\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiRNR-7Hty4T"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhvDstJxty4T"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN8W6hgbty4T",
        "outputId": "c6102c99-4017-4301-f21b-b4e66ed61b3e"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example).numpy())\n",
        "    print(\"Target: \",text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  b'1. Hit The Lights\\r\\n\\r\\n[James Hetfield]\\r\\n[Lars Ulrich]\\r\\n\\r\\nNo life till leather\\r\\nWe are gonna kick some'\n",
            "Target:  b'. Hit The Lights\\r\\n\\r\\n[James Hetfield]\\r\\n[Lars Ulrich]\\r\\n\\r\\nNo life till leather\\r\\nWe are gonna kick some '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOI5zuisty4U"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset\n",
        "           .shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE, drop_remainder=True)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-lnPF2Bty4U",
        "outputId": "4667e072-926b-4540-b515-0da1fe328132"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7vrHBMDty4U"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u49J_CU-ty4V"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "        \n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhebLNRCty4V"
      },
      "source": [
        "model = MyModel(vocab_size=len(ids_from_chars.get_vocabulary()), embedding_dim=embedding_dim, rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0OrUXKQty4V",
        "outputId": "026d2cbd-f80b-4d58-a0f5-4c030b9d445c"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print (example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 90) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN_YGX3Pty4W",
        "outputId": "83436d60-dcde-4989-a2ac-e8c6736525c5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  23040     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  92250     \n",
            "=================================================================\n",
            "Total params: 4,053,594\n",
            "Trainable params: 4,053,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ6FOSXyty4W"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oURdIHRUty4W",
        "outputId": "b5ada397-7c89-4202-a9b7-18ffd7590253"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56, 23, 81, 25, 66, 89, 58, 28, 85,  1, 41, 83, 60, 17, 71, 73, 54,\n",
              "       35, 82, 45, 70, 52,  2, 15, 40, 77, 22, 71, 55, 11, 33, 78, 54, 49,\n",
              "       60, 62, 38, 71, 51, 68, 10,  1, 45,  7, 50, 14, 73, 11, 80, 67, 64,\n",
              "       30, 20, 38, 14, 26, 79, 51,  0, 81, 87, 41,  1, 89,  7, 83, 73, 85,\n",
              "       16, 18, 23, 69, 75, 36, 42, 38, 49,  1,  1, 20,  1, 19, 58, 63, 55,\n",
              "        1, 39, 19, 18,  1, 70, 26, 55, 29, 53,  0, 30,  3, 82,  2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKWb47grty4W",
        "outputId": "972af903-95d1-43e2-de5d-c16d27b1e872"
      },
      "source": [
        "print (\"input: \\n\", text_from_ids(input_example_batch[0].numpy()))\n",
        "print ()\n",
        "print (\"Next Char Predictions: \\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: \n",
            " tf.Tensor(b' will be heard)\\r\\n\\r\\nThrough the worst we prevail\\r\\nSo our voices will be heard\\r\\nThrough the worst we p', shape=(), dtype=string)\n",
            "\n",
            "Next Char Predictions: \n",
            " b']7w9h\\xe2\\x9f\\x93`>\\xc3\\xa9\\nLyb1moZFxPlW\\r/Ks6m[*DtZTbdImVj)\\nP&U.o*vifA4I.:uV[UNK]w\\xe2\\x80\\x99L\\n\\xe2\\x9f\\x93&yo\\xc3\\xa9027kqGMIT\\n\\n4\\n3`e[\\nJ32\\nl:[?Y[UNK]A x\\r'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwXIGr3Ity4X"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56Qsl9IYty4X",
        "outputId": "35365886-ed88-4803-ec7f-608402139696"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print (\"Prediction shape: \", example_batch_predictions.shape, \"# (batch_size, sequence_lenght, vocab_size)\")\n",
        "print (\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 90) # (batch_size, sequence_lenght, vocab_size)\n",
            "Mean loss:         4.4996147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuKDl9kTty4X",
        "outputId": "2061fb9c-c6d9-4f8d-9dd9-20fbf3a71b69"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89.98245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoLNoAzQty4X"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQVXD2X_ty4Y",
        "outputId": "2ac534e0-dad0-4180-b996-1f7c4751d3a8"
      },
      "source": [
        "vocab_size   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBNPxdV_ty4Y"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO8bfEvYty4Y"
      },
      "source": [
        "#directory where checkpoints are saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "#name of checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbYJRd02ty4Y",
        "outputId": "fa76029f-3444-4917-d44f-40d63481c9a9"
      },
      "source": [
        "EPOCHS = 600\n",
        "\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "33/33 [==============================] - 4s 56ms/step - loss: 3.8523 - accuracy: 0.1574\n",
            "Epoch 2/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 2.7380 - accuracy: 0.2829\n",
            "Epoch 3/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 2.3601 - accuracy: 0.3424\n",
            "Epoch 4/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 2.2321 - accuracy: 0.3616\n",
            "Epoch 5/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 2.1399 - accuracy: 0.3805\n",
            "Epoch 6/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 2.0544 - accuracy: 0.3998\n",
            "Epoch 7/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 1.9728 - accuracy: 0.4207\n",
            "Epoch 8/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 1.8942 - accuracy: 0.4404\n",
            "Epoch 9/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.8191 - accuracy: 0.4611\n",
            "Epoch 10/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.7484 - accuracy: 0.4795\n",
            "Epoch 11/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.6827 - accuracy: 0.4975\n",
            "Epoch 12/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.6164 - accuracy: 0.5152\n",
            "Epoch 13/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.5514 - accuracy: 0.5327\n",
            "Epoch 14/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.4905 - accuracy: 0.5496\n",
            "Epoch 15/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.4290 - accuracy: 0.5674\n",
            "Epoch 16/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.3661 - accuracy: 0.5866\n",
            "Epoch 17/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.3024 - accuracy: 0.6054\n",
            "Epoch 18/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.2418 - accuracy: 0.6247\n",
            "Epoch 19/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.1761 - accuracy: 0.6446\n",
            "Epoch 20/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.1114 - accuracy: 0.6645\n",
            "Epoch 21/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.0449 - accuracy: 0.6859\n",
            "Epoch 22/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.9759 - accuracy: 0.7073\n",
            "Epoch 23/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.9022 - accuracy: 0.7318\n",
            "Epoch 24/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.8311 - accuracy: 0.7545\n",
            "Epoch 25/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.7595 - accuracy: 0.7776\n",
            "Epoch 26/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.6845 - accuracy: 0.8021\n",
            "Epoch 27/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.6123 - accuracy: 0.8262\n",
            "Epoch 28/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.5418 - accuracy: 0.8496\n",
            "Epoch 29/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4747 - accuracy: 0.8715\n",
            "Epoch 30/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4150 - accuracy: 0.8915\n",
            "Epoch 31/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.3557 - accuracy: 0.9115\n",
            "Epoch 32/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3017 - accuracy: 0.9293\n",
            "Epoch 33/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2595 - accuracy: 0.9418\n",
            "Epoch 34/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.2221 - accuracy: 0.9526\n",
            "Epoch 35/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1941 - accuracy: 0.9600\n",
            "Epoch 36/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.1711 - accuracy: 0.9648\n",
            "Epoch 37/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1515 - accuracy: 0.9694\n",
            "Epoch 38/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1363 - accuracy: 0.9715\n",
            "Epoch 39/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1244 - accuracy: 0.9735\n",
            "Epoch 40/600\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.1150 - accuracy: 0.9745\n",
            "Epoch 41/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1068 - accuracy: 0.9756\n",
            "Epoch 42/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1011 - accuracy: 0.9760\n",
            "Epoch 43/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0959 - accuracy: 0.9767\n",
            "Epoch 44/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0928 - accuracy: 0.9771\n",
            "Epoch 45/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0889 - accuracy: 0.9774\n",
            "Epoch 46/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0861 - accuracy: 0.9777\n",
            "Epoch 47/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0837 - accuracy: 0.9778\n",
            "Epoch 48/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0815 - accuracy: 0.9783\n",
            "Epoch 49/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0805 - accuracy: 0.9781\n",
            "Epoch 50/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0791 - accuracy: 0.9781\n",
            "Epoch 51/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0785 - accuracy: 0.9784\n",
            "Epoch 52/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0774 - accuracy: 0.9783\n",
            "Epoch 53/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0764 - accuracy: 0.9785\n",
            "Epoch 54/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0769 - accuracy: 0.9784\n",
            "Epoch 55/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0758 - accuracy: 0.9783\n",
            "Epoch 56/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0751 - accuracy: 0.9785\n",
            "Epoch 57/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0743 - accuracy: 0.9786\n",
            "Epoch 58/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0731 - accuracy: 0.9786\n",
            "Epoch 59/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0732 - accuracy: 0.9786\n",
            "Epoch 60/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0728 - accuracy: 0.9787\n",
            "Epoch 61/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0720 - accuracy: 0.9785\n",
            "Epoch 62/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0717 - accuracy: 0.9788\n",
            "Epoch 63/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0720 - accuracy: 0.9786\n",
            "Epoch 64/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0717 - accuracy: 0.9787\n",
            "Epoch 65/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0718 - accuracy: 0.9786\n",
            "Epoch 66/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0725 - accuracy: 0.9785\n",
            "Epoch 67/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0733 - accuracy: 0.9783\n",
            "Epoch 68/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0745 - accuracy: 0.9783\n",
            "Epoch 69/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0763 - accuracy: 0.9776\n",
            "Epoch 70/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0792 - accuracy: 0.9771\n",
            "Epoch 71/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0849 - accuracy: 0.9757\n",
            "Epoch 72/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0956 - accuracy: 0.9728\n",
            "Epoch 73/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1138 - accuracy: 0.9665\n",
            "Epoch 74/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1406 - accuracy: 0.9575\n",
            "Epoch 75/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1639 - accuracy: 0.9492\n",
            "Epoch 76/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1606 - accuracy: 0.9498\n",
            "Epoch 77/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1337 - accuracy: 0.9595\n",
            "Epoch 78/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1069 - accuracy: 0.9688\n",
            "Epoch 79/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0890 - accuracy: 0.9743\n",
            "Epoch 80/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0767 - accuracy: 0.9774\n",
            "Epoch 81/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0694 - accuracy: 0.9784\n",
            "Epoch 82/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0661 - accuracy: 0.9792\n",
            "Epoch 83/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0639 - accuracy: 0.9793\n",
            "Epoch 84/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0632 - accuracy: 0.9792\n",
            "Epoch 85/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0623 - accuracy: 0.9794\n",
            "Epoch 86/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0619 - accuracy: 0.9796\n",
            "Epoch 87/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0614 - accuracy: 0.9794\n",
            "Epoch 88/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0605 - accuracy: 0.9795\n",
            "Epoch 89/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0601 - accuracy: 0.9797\n",
            "Epoch 90/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0607 - accuracy: 0.9794\n",
            "Epoch 91/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0602 - accuracy: 0.9796\n",
            "Epoch 92/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0597 - accuracy: 0.9796\n",
            "Epoch 93/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0601 - accuracy: 0.9796\n",
            "Epoch 94/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0598 - accuracy: 0.9796\n",
            "Epoch 95/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0588 - accuracy: 0.9799\n",
            "Epoch 96/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0591 - accuracy: 0.9798\n",
            "Epoch 97/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0590 - accuracy: 0.9797\n",
            "Epoch 98/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0590 - accuracy: 0.9798\n",
            "Epoch 99/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0592 - accuracy: 0.9797\n",
            "Epoch 100/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0597 - accuracy: 0.9798\n",
            "Epoch 101/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0591 - accuracy: 0.9796\n",
            "Epoch 102/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0593 - accuracy: 0.9797\n",
            "Epoch 103/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0592 - accuracy: 0.9796\n",
            "Epoch 104/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0588 - accuracy: 0.9797\n",
            "Epoch 105/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0589 - accuracy: 0.9798\n",
            "Epoch 106/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0587 - accuracy: 0.9797\n",
            "Epoch 107/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0584 - accuracy: 0.9799\n",
            "Epoch 108/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0586 - accuracy: 0.9797\n",
            "Epoch 109/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0588 - accuracy: 0.9798\n",
            "Epoch 110/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0588 - accuracy: 0.9796\n",
            "Epoch 111/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0588 - accuracy: 0.9796\n",
            "Epoch 112/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0583 - accuracy: 0.9797\n",
            "Epoch 113/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0583 - accuracy: 0.9797\n",
            "Epoch 114/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0591 - accuracy: 0.9796\n",
            "Epoch 115/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0585 - accuracy: 0.9796\n",
            "Epoch 116/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0594 - accuracy: 0.9797\n",
            "Epoch 117/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0595 - accuracy: 0.9796\n",
            "Epoch 118/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0597 - accuracy: 0.9796\n",
            "Epoch 119/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0598 - accuracy: 0.9796\n",
            "Epoch 120/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0602 - accuracy: 0.9796\n",
            "Epoch 121/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0608 - accuracy: 0.9795\n",
            "Epoch 122/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0614 - accuracy: 0.9794\n",
            "Epoch 123/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0637 - accuracy: 0.9790\n",
            "Epoch 124/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0668 - accuracy: 0.9783\n",
            "Epoch 125/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0770 - accuracy: 0.9760\n",
            "Epoch 126/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1386 - accuracy: 0.9559\n",
            "Epoch 127/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4024 - accuracy: 0.8702\n",
            "Epoch 128/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4776 - accuracy: 0.8449\n",
            "Epoch 129/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3378 - accuracy: 0.8875\n",
            "Epoch 130/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2201 - accuracy: 0.9279\n",
            "Epoch 131/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1493 - accuracy: 0.9536\n",
            "Epoch 132/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1088 - accuracy: 0.9681\n",
            "Epoch 133/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0864 - accuracy: 0.9742\n",
            "Epoch 134/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0742 - accuracy: 0.9773\n",
            "Epoch 135/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0672 - accuracy: 0.9784\n",
            "Epoch 136/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0635 - accuracy: 0.9790\n",
            "Epoch 137/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0609 - accuracy: 0.9793\n",
            "Epoch 138/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0594 - accuracy: 0.9793\n",
            "Epoch 139/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0587 - accuracy: 0.9795\n",
            "Epoch 140/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0581 - accuracy: 0.9795\n",
            "Epoch 141/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0578 - accuracy: 0.9797\n",
            "Epoch 142/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0571 - accuracy: 0.9799\n",
            "Epoch 143/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0569 - accuracy: 0.9799\n",
            "Epoch 144/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0567 - accuracy: 0.9798\n",
            "Epoch 145/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0565 - accuracy: 0.9802\n",
            "Epoch 146/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0559 - accuracy: 0.9799\n",
            "Epoch 147/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0559 - accuracy: 0.9799\n",
            "Epoch 148/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0556 - accuracy: 0.9799\n",
            "Epoch 149/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0557 - accuracy: 0.9798\n",
            "Epoch 150/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0555 - accuracy: 0.9802\n",
            "Epoch 151/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0563 - accuracy: 0.9801\n",
            "Epoch 152/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0563 - accuracy: 0.9798\n",
            "Epoch 153/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0564 - accuracy: 0.9799\n",
            "Epoch 154/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0561 - accuracy: 0.9798\n",
            "Epoch 155/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0560 - accuracy: 0.9800\n",
            "Epoch 156/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0557 - accuracy: 0.9801\n",
            "Epoch 157/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0557 - accuracy: 0.9800\n",
            "Epoch 158/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0559 - accuracy: 0.9798\n",
            "Epoch 159/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0556 - accuracy: 0.9800\n",
            "Epoch 160/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9802\n",
            "Epoch 161/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0555 - accuracy: 0.9799\n",
            "Epoch 162/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9800\n",
            "Epoch 163/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0549 - accuracy: 0.9802\n",
            "Epoch 164/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0556 - accuracy: 0.9799\n",
            "Epoch 165/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9800\n",
            "Epoch 166/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0559 - accuracy: 0.9799\n",
            "Epoch 167/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0555 - accuracy: 0.9803\n",
            "Epoch 168/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0556 - accuracy: 0.9801\n",
            "Epoch 169/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0557 - accuracy: 0.9801\n",
            "Epoch 170/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0553 - accuracy: 0.9799\n",
            "Epoch 171/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0556 - accuracy: 0.9799\n",
            "Epoch 172/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0553 - accuracy: 0.9800\n",
            "Epoch 173/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0556 - accuracy: 0.9800\n",
            "Epoch 174/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0556 - accuracy: 0.9798\n",
            "Epoch 175/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9800\n",
            "Epoch 176/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9801\n",
            "Epoch 177/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0556 - accuracy: 0.9801\n",
            "Epoch 178/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0562 - accuracy: 0.9800\n",
            "Epoch 179/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0564 - accuracy: 0.9800\n",
            "Epoch 180/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0564 - accuracy: 0.9796\n",
            "Epoch 181/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0566 - accuracy: 0.9799\n",
            "Epoch 182/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0569 - accuracy: 0.9799\n",
            "Epoch 183/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0576 - accuracy: 0.9796\n",
            "Epoch 184/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0596 - accuracy: 0.9796\n",
            "Epoch 185/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0635 - accuracy: 0.9787\n",
            "Epoch 186/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0693 - accuracy: 0.9777\n",
            "Epoch 187/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0940 - accuracy: 0.9704\n",
            "Epoch 188/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2369 - accuracy: 0.9217\n",
            "Epoch 189/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5264 - accuracy: 0.8347\n",
            "Epoch 190/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.5501 - accuracy: 0.8233\n",
            "Epoch 191/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4114 - accuracy: 0.8627\n",
            "Epoch 192/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2978 - accuracy: 0.9005\n",
            "Epoch 193/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2159 - accuracy: 0.9291\n",
            "Epoch 194/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1607 - accuracy: 0.9488\n",
            "Epoch 195/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1241 - accuracy: 0.9622\n",
            "Epoch 196/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0986 - accuracy: 0.9709\n",
            "Epoch 197/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0814 - accuracy: 0.9758\n",
            "Epoch 198/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0711 - accuracy: 0.9781\n",
            "Epoch 199/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0651 - accuracy: 0.9790\n",
            "Epoch 200/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0616 - accuracy: 0.9792\n",
            "Epoch 201/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0592 - accuracy: 0.9797\n",
            "Epoch 202/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0573 - accuracy: 0.9800\n",
            "Epoch 203/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0570 - accuracy: 0.9799\n",
            "Epoch 204/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0561 - accuracy: 0.9800\n",
            "Epoch 205/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0556 - accuracy: 0.9799\n",
            "Epoch 206/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0551 - accuracy: 0.9803\n",
            "Epoch 207/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0551 - accuracy: 0.9803\n",
            "Epoch 208/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0551 - accuracy: 0.9802\n",
            "Epoch 209/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0549 - accuracy: 0.9800\n",
            "Epoch 210/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0548 - accuracy: 0.9801\n",
            "Epoch 211/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0546 - accuracy: 0.9802\n",
            "Epoch 212/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0540 - accuracy: 0.9803\n",
            "Epoch 213/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0545 - accuracy: 0.9803\n",
            "Epoch 214/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0543 - accuracy: 0.9803\n",
            "Epoch 215/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0541 - accuracy: 0.9804\n",
            "Epoch 216/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0542 - accuracy: 0.9801\n",
            "Epoch 217/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0543 - accuracy: 0.9804\n",
            "Epoch 218/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0543 - accuracy: 0.9802\n",
            "Epoch 219/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0543 - accuracy: 0.9801\n",
            "Epoch 220/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0541 - accuracy: 0.9801\n",
            "Epoch 221/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 222/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 223/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0538 - accuracy: 0.9803\n",
            "Epoch 224/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0540 - accuracy: 0.9803\n",
            "Epoch 225/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0540 - accuracy: 0.9802\n",
            "Epoch 226/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0542 - accuracy: 0.9802\n",
            "Epoch 227/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 228/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 229/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 230/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0540 - accuracy: 0.9803\n",
            "Epoch 231/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0541 - accuracy: 0.9801\n",
            "Epoch 232/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0541 - accuracy: 0.9803\n",
            "Epoch 233/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0537 - accuracy: 0.9803\n",
            "Epoch 234/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0540 - accuracy: 0.9802\n",
            "Epoch 235/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0542 - accuracy: 0.9803\n",
            "Epoch 236/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0542 - accuracy: 0.9802\n",
            "Epoch 237/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0541 - accuracy: 0.9804\n",
            "Epoch 238/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0543 - accuracy: 0.9800\n",
            "Epoch 239/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0542 - accuracy: 0.9802\n",
            "Epoch 240/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0548 - accuracy: 0.9801\n",
            "Epoch 241/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0555 - accuracy: 0.9799\n",
            "Epoch 242/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0555 - accuracy: 0.9801\n",
            "Epoch 243/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0567 - accuracy: 0.9798\n",
            "Epoch 244/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0582 - accuracy: 0.9797\n",
            "Epoch 245/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0617 - accuracy: 0.9791\n",
            "Epoch 246/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0668 - accuracy: 0.9780\n",
            "Epoch 247/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0772 - accuracy: 0.9758\n",
            "Epoch 248/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1244 - accuracy: 0.9601\n",
            "Epoch 249/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3551 - accuracy: 0.8844\n",
            "Epoch 250/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.7073 - accuracy: 0.7867\n",
            "Epoch 251/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.7335 - accuracy: 0.7751\n",
            "Epoch 252/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.6100 - accuracy: 0.8047\n",
            "Epoch 253/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5041 - accuracy: 0.8330\n",
            "Epoch 254/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4156 - accuracy: 0.8606\n",
            "Epoch 255/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3405 - accuracy: 0.8852\n",
            "Epoch 256/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2856 - accuracy: 0.9043\n",
            "Epoch 257/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2424 - accuracy: 0.9195\n",
            "Epoch 258/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1992 - accuracy: 0.9357\n",
            "Epoch 259/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1659 - accuracy: 0.9485\n",
            "Epoch 260/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1366 - accuracy: 0.9594\n",
            "Epoch 261/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1122 - accuracy: 0.9681\n",
            "Epoch 262/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0951 - accuracy: 0.9736\n",
            "Epoch 263/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0825 - accuracy: 0.9766\n",
            "Epoch 264/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0731 - accuracy: 0.9785\n",
            "Epoch 265/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0670 - accuracy: 0.9792\n",
            "Epoch 266/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0627 - accuracy: 0.9796\n",
            "Epoch 267/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0597 - accuracy: 0.9800\n",
            "Epoch 268/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0582 - accuracy: 0.9801\n",
            "Epoch 269/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0570 - accuracy: 0.9802\n",
            "Epoch 270/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0559 - accuracy: 0.9802\n",
            "Epoch 271/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0551 - accuracy: 0.9806\n",
            "Epoch 272/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0550 - accuracy: 0.9804\n",
            "Epoch 273/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0546 - accuracy: 0.9806\n",
            "Epoch 274/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0544 - accuracy: 0.9806\n",
            "Epoch 275/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0540 - accuracy: 0.9806\n",
            "Epoch 276/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0538 - accuracy: 0.9806\n",
            "Epoch 277/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0536 - accuracy: 0.9806\n",
            "Epoch 278/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0534 - accuracy: 0.9805\n",
            "Epoch 279/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0534 - accuracy: 0.9806\n",
            "Epoch 280/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0530 - accuracy: 0.9806\n",
            "Epoch 281/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0530 - accuracy: 0.9807\n",
            "Epoch 282/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0528 - accuracy: 0.9807\n",
            "Epoch 283/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0529 - accuracy: 0.9806\n",
            "Epoch 284/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9807\n",
            "Epoch 285/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9809\n",
            "Epoch 286/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0526 - accuracy: 0.9806\n",
            "Epoch 287/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0526 - accuracy: 0.9807\n",
            "Epoch 288/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9807\n",
            "Epoch 289/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0525 - accuracy: 0.9807\n",
            "Epoch 290/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0525 - accuracy: 0.9807\n",
            "Epoch 291/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0524 - accuracy: 0.9808\n",
            "Epoch 292/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9806\n",
            "Epoch 293/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0528 - accuracy: 0.9807\n",
            "Epoch 294/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9806\n",
            "Epoch 295/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0526 - accuracy: 0.9805\n",
            "Epoch 296/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0529 - accuracy: 0.9805\n",
            "Epoch 297/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0529 - accuracy: 0.9803\n",
            "Epoch 298/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0527 - accuracy: 0.9804\n",
            "Epoch 299/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0530 - accuracy: 0.9806\n",
            "Epoch 300/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0532 - accuracy: 0.9805\n",
            "Epoch 301/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0538 - accuracy: 0.9803\n",
            "Epoch 302/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0541 - accuracy: 0.9803\n",
            "Epoch 303/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0544 - accuracy: 0.9803\n",
            "Epoch 304/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0548 - accuracy: 0.9802\n",
            "Epoch 305/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0557 - accuracy: 0.9801\n",
            "Epoch 306/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0565 - accuracy: 0.9801\n",
            "Epoch 307/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0584 - accuracy: 0.9796\n",
            "Epoch 308/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0616 - accuracy: 0.9790\n",
            "Epoch 309/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0666 - accuracy: 0.9781\n",
            "Epoch 310/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0745 - accuracy: 0.9762\n",
            "Epoch 311/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1011 - accuracy: 0.9678\n",
            "Epoch 312/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2573 - accuracy: 0.9142\n",
            "Epoch 313/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.8081 - accuracy: 0.7686\n",
            "Epoch 314/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 1.2835 - accuracy: 0.6619\n",
            "Epoch 315/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.3045 - accuracy: 0.6474\n",
            "Epoch 316/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.1070 - accuracy: 0.6781\n",
            "Epoch 317/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.9210 - accuracy: 0.7193\n",
            "Epoch 318/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.7946 - accuracy: 0.7485\n",
            "Epoch 319/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.7039 - accuracy: 0.7726\n",
            "Epoch 320/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.6277 - accuracy: 0.7942\n",
            "Epoch 321/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5723 - accuracy: 0.8114\n",
            "Epoch 322/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5351 - accuracy: 0.8224\n",
            "Epoch 323/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.4918 - accuracy: 0.8365\n",
            "Epoch 324/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4516 - accuracy: 0.8513\n",
            "Epoch 325/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.4208 - accuracy: 0.8606\n",
            "Epoch 326/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3982 - accuracy: 0.8683\n",
            "Epoch 327/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3707 - accuracy: 0.8782\n",
            "Epoch 328/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3472 - accuracy: 0.8860\n",
            "Epoch 329/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.3315 - accuracy: 0.8916\n",
            "Epoch 330/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3138 - accuracy: 0.8981\n",
            "Epoch 331/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2956 - accuracy: 0.9043\n",
            "Epoch 332/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2766 - accuracy: 0.9124\n",
            "Epoch 333/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2592 - accuracy: 0.9182\n",
            "Epoch 334/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2445 - accuracy: 0.9240\n",
            "Epoch 335/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2319 - accuracy: 0.9276\n",
            "Epoch 336/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2164 - accuracy: 0.9340\n",
            "Epoch 337/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.2004 - accuracy: 0.9401\n",
            "Epoch 338/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1820 - accuracy: 0.9470\n",
            "Epoch 339/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1683 - accuracy: 0.9522\n",
            "Epoch 340/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.1537 - accuracy: 0.9572\n",
            "Epoch 341/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1383 - accuracy: 0.9631\n",
            "Epoch 342/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1242 - accuracy: 0.9678\n",
            "Epoch 343/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1124 - accuracy: 0.9716\n",
            "Epoch 344/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1003 - accuracy: 0.9752\n",
            "Epoch 345/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0908 - accuracy: 0.9773\n",
            "Epoch 346/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0826 - accuracy: 0.9787\n",
            "Epoch 347/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0768 - accuracy: 0.9792\n",
            "Epoch 348/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0720 - accuracy: 0.9799\n",
            "Epoch 349/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0681 - accuracy: 0.9803\n",
            "Epoch 350/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0652 - accuracy: 0.9802\n",
            "Epoch 351/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0627 - accuracy: 0.9806\n",
            "Epoch 352/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0607 - accuracy: 0.9808\n",
            "Epoch 353/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0590 - accuracy: 0.9808\n",
            "Epoch 354/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0580 - accuracy: 0.9808\n",
            "Epoch 355/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0572 - accuracy: 0.9808\n",
            "Epoch 356/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0563 - accuracy: 0.9808\n",
            "Epoch 357/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0559 - accuracy: 0.9811\n",
            "Epoch 358/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0554 - accuracy: 0.9809\n",
            "Epoch 359/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0550 - accuracy: 0.9810\n",
            "Epoch 360/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0548 - accuracy: 0.9810\n",
            "Epoch 361/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0544 - accuracy: 0.9810\n",
            "Epoch 362/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0542 - accuracy: 0.9810\n",
            "Epoch 363/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0538 - accuracy: 0.9811\n",
            "Epoch 364/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0537 - accuracy: 0.9811\n",
            "Epoch 365/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0535 - accuracy: 0.9810\n",
            "Epoch 366/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0533 - accuracy: 0.9810\n",
            "Epoch 367/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0530 - accuracy: 0.9810\n",
            "Epoch 368/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0529 - accuracy: 0.9812\n",
            "Epoch 369/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0531 - accuracy: 0.9809\n",
            "Epoch 370/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0530 - accuracy: 0.9809\n",
            "Epoch 371/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0527 - accuracy: 0.9810\n",
            "Epoch 372/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0528 - accuracy: 0.9811\n",
            "Epoch 373/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0527 - accuracy: 0.9810\n",
            "Epoch 374/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0525 - accuracy: 0.9810\n",
            "Epoch 375/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0524 - accuracy: 0.9810\n",
            "Epoch 376/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0524 - accuracy: 0.9810\n",
            "Epoch 377/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0522 - accuracy: 0.9811\n",
            "Epoch 378/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0523 - accuracy: 0.9809\n",
            "Epoch 379/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0522 - accuracy: 0.9809\n",
            "Epoch 380/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0523 - accuracy: 0.9809\n",
            "Epoch 381/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0525 - accuracy: 0.9809\n",
            "Epoch 382/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0525 - accuracy: 0.9809\n",
            "Epoch 383/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0527 - accuracy: 0.9810\n",
            "Epoch 384/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0526 - accuracy: 0.9809\n",
            "Epoch 385/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0526 - accuracy: 0.9808\n",
            "Epoch 386/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0527 - accuracy: 0.9809\n",
            "Epoch 387/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0532 - accuracy: 0.9807\n",
            "Epoch 388/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0538 - accuracy: 0.9806\n",
            "Epoch 389/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0543 - accuracy: 0.9805\n",
            "Epoch 390/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0550 - accuracy: 0.9805\n",
            "Epoch 391/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0559 - accuracy: 0.9803\n",
            "Epoch 392/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0576 - accuracy: 0.9800\n",
            "Epoch 393/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0599 - accuracy: 0.9796\n",
            "Epoch 394/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0655 - accuracy: 0.9788\n",
            "Epoch 395/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0798 - accuracy: 0.9757\n",
            "Epoch 396/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1566 - accuracy: 0.9488\n",
            "Epoch 397/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.8516 - accuracy: 0.7648\n",
            "Epoch 398/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.1498 - accuracy: 0.5369\n",
            "Epoch 399/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.2199 - accuracy: 0.4971\n",
            "Epoch 400/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.8878 - accuracy: 0.5226\n",
            "Epoch 401/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.5869 - accuracy: 0.5638\n",
            "Epoch 402/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.3817 - accuracy: 0.6016\n",
            "Epoch 403/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.2223 - accuracy: 0.6357\n",
            "Epoch 404/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.1037 - accuracy: 0.6646\n",
            "Epoch 405/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.0017 - accuracy: 0.6929\n",
            "Epoch 406/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.9157 - accuracy: 0.7179\n",
            "Epoch 407/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.8423 - accuracy: 0.7406\n",
            "Epoch 408/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.7773 - accuracy: 0.7619\n",
            "Epoch 409/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.7221 - accuracy: 0.7811\n",
            "Epoch 410/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.6715 - accuracy: 0.7977\n",
            "Epoch 411/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.6230 - accuracy: 0.8148\n",
            "Epoch 412/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5810 - accuracy: 0.8293\n",
            "Epoch 413/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5403 - accuracy: 0.8432\n",
            "Epoch 414/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5066 - accuracy: 0.8553\n",
            "Epoch 415/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4722 - accuracy: 0.8667\n",
            "Epoch 416/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.4427 - accuracy: 0.8776\n",
            "Epoch 417/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4121 - accuracy: 0.8884\n",
            "Epoch 418/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3844 - accuracy: 0.8986\n",
            "Epoch 419/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3598 - accuracy: 0.9070\n",
            "Epoch 420/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3366 - accuracy: 0.9155\n",
            "Epoch 421/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3146 - accuracy: 0.9233\n",
            "Epoch 422/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2961 - accuracy: 0.9293\n",
            "Epoch 423/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2767 - accuracy: 0.9354\n",
            "Epoch 424/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2579 - accuracy: 0.9422\n",
            "Epoch 425/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2372 - accuracy: 0.9496\n",
            "Epoch 426/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2188 - accuracy: 0.9556\n",
            "Epoch 427/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2021 - accuracy: 0.9603\n",
            "Epoch 428/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1866 - accuracy: 0.9648\n",
            "Epoch 429/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1743 - accuracy: 0.9680\n",
            "Epoch 430/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1617 - accuracy: 0.9714\n",
            "Epoch 431/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1510 - accuracy: 0.9730\n",
            "Epoch 432/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1406 - accuracy: 0.9752\n",
            "Epoch 433/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1313 - accuracy: 0.9766\n",
            "Epoch 434/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1228 - accuracy: 0.9779\n",
            "Epoch 435/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1147 - accuracy: 0.9788\n",
            "Epoch 436/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1079 - accuracy: 0.9794\n",
            "Epoch 437/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1021 - accuracy: 0.9798\n",
            "Epoch 438/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0967 - accuracy: 0.9802\n",
            "Epoch 439/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0923 - accuracy: 0.9804\n",
            "Epoch 440/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0885 - accuracy: 0.9804\n",
            "Epoch 441/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0855 - accuracy: 0.9807\n",
            "Epoch 442/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0825 - accuracy: 0.9807\n",
            "Epoch 443/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0796 - accuracy: 0.9808\n",
            "Epoch 444/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0774 - accuracy: 0.9810\n",
            "Epoch 445/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0753 - accuracy: 0.9810\n",
            "Epoch 446/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0736 - accuracy: 0.9809\n",
            "Epoch 447/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0722 - accuracy: 0.9808\n",
            "Epoch 448/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0706 - accuracy: 0.9810\n",
            "Epoch 449/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0694 - accuracy: 0.9811\n",
            "Epoch 450/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0682 - accuracy: 0.9811\n",
            "Epoch 451/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0673 - accuracy: 0.9810\n",
            "Epoch 452/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0662 - accuracy: 0.9810\n",
            "Epoch 453/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0657 - accuracy: 0.9809\n",
            "Epoch 454/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0652 - accuracy: 0.9810\n",
            "Epoch 455/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0644 - accuracy: 0.9810\n",
            "Epoch 456/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0640 - accuracy: 0.9809\n",
            "Epoch 457/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0636 - accuracy: 0.9809\n",
            "Epoch 458/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0629 - accuracy: 0.9807\n",
            "Epoch 459/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0625 - accuracy: 0.9808\n",
            "Epoch 460/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0624 - accuracy: 0.9807\n",
            "Epoch 461/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0620 - accuracy: 0.9808\n",
            "Epoch 462/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0614 - accuracy: 0.9810\n",
            "Epoch 463/600\n",
            "33/33 [==============================] - 2s 55ms/step - loss: 0.0614 - accuracy: 0.9808\n",
            "Epoch 464/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0609 - accuracy: 0.9809\n",
            "Epoch 465/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0613 - accuracy: 0.9809\n",
            "Epoch 466/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0610 - accuracy: 0.9807\n",
            "Epoch 467/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0615 - accuracy: 0.9806\n",
            "Epoch 468/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0623 - accuracy: 0.9805\n",
            "Epoch 469/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0639 - accuracy: 0.9805\n",
            "Epoch 470/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0657 - accuracy: 0.9801\n",
            "Epoch 471/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0683 - accuracy: 0.9800\n",
            "Epoch 472/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0731 - accuracy: 0.9794\n",
            "Epoch 473/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0857 - accuracy: 0.9772\n",
            "Epoch 474/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1789 - accuracy: 0.9462\n",
            "Epoch 475/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.0662 - accuracy: 0.7094\n",
            "Epoch 476/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 2.1107 - accuracy: 0.5356\n",
            "Epoch 477/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.2476 - accuracy: 0.5033\n",
            "Epoch 478/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.2635 - accuracy: 0.4879\n",
            "Epoch 479/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.1308 - accuracy: 0.4901\n",
            "Epoch 480/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.9153 - accuracy: 0.5101\n",
            "Epoch 481/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.6866 - accuracy: 0.5419\n",
            "Epoch 482/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.4981 - accuracy: 0.5749\n",
            "Epoch 483/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.3614 - accuracy: 0.6027\n",
            "Epoch 484/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.2511 - accuracy: 0.6290\n",
            "Epoch 485/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.1538 - accuracy: 0.6529\n",
            "Epoch 486/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.0661 - accuracy: 0.6782\n",
            "Epoch 487/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.9965 - accuracy: 0.6979\n",
            "Epoch 488/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.9324 - accuracy: 0.7175\n",
            "Epoch 489/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.8776 - accuracy: 0.7345\n",
            "Epoch 490/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.8276 - accuracy: 0.7508\n",
            "Epoch 491/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.7807 - accuracy: 0.7675\n",
            "Epoch 492/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.7361 - accuracy: 0.7812\n",
            "Epoch 493/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.6999 - accuracy: 0.7931\n",
            "Epoch 494/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.6660 - accuracy: 0.8055\n",
            "Epoch 495/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.6316 - accuracy: 0.8163\n",
            "Epoch 496/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.6005 - accuracy: 0.8280\n",
            "Epoch 497/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5709 - accuracy: 0.8376\n",
            "Epoch 498/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5455 - accuracy: 0.8463\n",
            "Epoch 499/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5206 - accuracy: 0.8554\n",
            "Epoch 500/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4946 - accuracy: 0.8642\n",
            "Epoch 501/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4712 - accuracy: 0.8727\n",
            "Epoch 502/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.4473 - accuracy: 0.8806\n",
            "Epoch 503/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4248 - accuracy: 0.8888\n",
            "Epoch 504/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.4034 - accuracy: 0.8959\n",
            "Epoch 505/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3816 - accuracy: 0.9040\n",
            "Epoch 506/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3618 - accuracy: 0.9110\n",
            "Epoch 507/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3461 - accuracy: 0.9160\n",
            "Epoch 508/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.3286 - accuracy: 0.9218\n",
            "Epoch 509/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.3115 - accuracy: 0.9275\n",
            "Epoch 510/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2964 - accuracy: 0.9325\n",
            "Epoch 511/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2803 - accuracy: 0.9381\n",
            "Epoch 512/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.2673 - accuracy: 0.9421\n",
            "Epoch 513/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2541 - accuracy: 0.9461\n",
            "Epoch 514/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.2402 - accuracy: 0.9508\n",
            "Epoch 515/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2281 - accuracy: 0.9538\n",
            "Epoch 516/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2141 - accuracy: 0.9588\n",
            "Epoch 517/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.2036 - accuracy: 0.9616\n",
            "Epoch 518/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1927 - accuracy: 0.9643\n",
            "Epoch 519/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1839 - accuracy: 0.9666\n",
            "Epoch 520/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1726 - accuracy: 0.9691\n",
            "Epoch 521/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.1634 - accuracy: 0.9715\n",
            "Epoch 522/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1555 - accuracy: 0.9729\n",
            "Epoch 523/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1465 - accuracy: 0.9746\n",
            "Epoch 524/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1374 - accuracy: 0.9762\n",
            "Epoch 525/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1296 - accuracy: 0.9772\n",
            "Epoch 526/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1226 - accuracy: 0.9781\n",
            "Epoch 527/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1159 - accuracy: 0.9787\n",
            "Epoch 528/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1102 - accuracy: 0.9792\n",
            "Epoch 529/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1058 - accuracy: 0.9795\n",
            "Epoch 530/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1006 - accuracy: 0.9800\n",
            "Epoch 531/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0966 - accuracy: 0.9801\n",
            "Epoch 532/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0936 - accuracy: 0.9802\n",
            "Epoch 533/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0901 - accuracy: 0.9805\n",
            "Epoch 534/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0870 - accuracy: 0.9805\n",
            "Epoch 535/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0849 - accuracy: 0.9805\n",
            "Epoch 536/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0825 - accuracy: 0.9807\n",
            "Epoch 537/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0806 - accuracy: 0.9807\n",
            "Epoch 538/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0792 - accuracy: 0.9808\n",
            "Epoch 539/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0772 - accuracy: 0.9807\n",
            "Epoch 540/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0756 - accuracy: 0.9807\n",
            "Epoch 541/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0748 - accuracy: 0.9808\n",
            "Epoch 542/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0741 - accuracy: 0.9809\n",
            "Epoch 543/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0730 - accuracy: 0.9808\n",
            "Epoch 544/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0725 - accuracy: 0.9807\n",
            "Epoch 545/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0716 - accuracy: 0.9808\n",
            "Epoch 546/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0704 - accuracy: 0.9808\n",
            "Epoch 547/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0697 - accuracy: 0.9808\n",
            "Epoch 548/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0691 - accuracy: 0.9807\n",
            "Epoch 549/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0685 - accuracy: 0.9808\n",
            "Epoch 550/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0678 - accuracy: 0.9808\n",
            "Epoch 551/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0677 - accuracy: 0.9806\n",
            "Epoch 552/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0675 - accuracy: 0.9806\n",
            "Epoch 553/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0676 - accuracy: 0.9806\n",
            "Epoch 554/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0681 - accuracy: 0.9805\n",
            "Epoch 555/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0687 - accuracy: 0.9805\n",
            "Epoch 556/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0695 - accuracy: 0.9803\n",
            "Epoch 557/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.0711 - accuracy: 0.9802\n",
            "Epoch 558/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.0739 - accuracy: 0.9800\n",
            "Epoch 559/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0784 - accuracy: 0.9795\n",
            "Epoch 560/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.0970 - accuracy: 0.9760\n",
            "Epoch 561/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.3362 - accuracy: 0.8942\n",
            "Epoch 562/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.6994 - accuracy: 0.5938\n",
            "Epoch 563/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.5122 - accuracy: 0.4801\n",
            "Epoch 564/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.6873 - accuracy: 0.4484\n",
            "Epoch 565/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 2.6954 - accuracy: 0.4318\n",
            "Epoch 566/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.4842 - accuracy: 0.4415\n",
            "Epoch 567/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.2277 - accuracy: 0.4610\n",
            "Epoch 568/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 2.0052 - accuracy: 0.4842\n",
            "Epoch 569/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 1.8089 - accuracy: 0.5106\n",
            "Epoch 570/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.6523 - accuracy: 0.5370\n",
            "Epoch 571/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.5308 - accuracy: 0.5600\n",
            "Epoch 572/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.4329 - accuracy: 0.5814\n",
            "Epoch 573/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.3560 - accuracy: 0.5991\n",
            "Epoch 574/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.2954 - accuracy: 0.6141\n",
            "Epoch 575/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 1.2282 - accuracy: 0.6333\n",
            "Epoch 576/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.1713 - accuracy: 0.6486\n",
            "Epoch 577/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 1.1219 - accuracy: 0.6642\n",
            "Epoch 578/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.0792 - accuracy: 0.6770\n",
            "Epoch 579/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.0422 - accuracy: 0.6892\n",
            "Epoch 580/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 1.0061 - accuracy: 0.7004\n",
            "Epoch 581/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.9676 - accuracy: 0.7119\n",
            "Epoch 582/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.9353 - accuracy: 0.7224\n",
            "Epoch 583/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.9033 - accuracy: 0.7334\n",
            "Epoch 584/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.8808 - accuracy: 0.7409\n",
            "Epoch 585/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.8539 - accuracy: 0.7495\n",
            "Epoch 586/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.8316 - accuracy: 0.7572\n",
            "Epoch 587/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.8088 - accuracy: 0.7643\n",
            "Epoch 588/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.7886 - accuracy: 0.7712\n",
            "Epoch 589/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.7609 - accuracy: 0.7801\n",
            "Epoch 590/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.7394 - accuracy: 0.7882\n",
            "Epoch 591/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.7163 - accuracy: 0.7962\n",
            "Epoch 592/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.7017 - accuracy: 0.8002\n",
            "Epoch 593/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.6821 - accuracy: 0.8070\n",
            "Epoch 594/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.6638 - accuracy: 0.8134\n",
            "Epoch 595/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.6460 - accuracy: 0.8189\n",
            "Epoch 596/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.6298 - accuracy: 0.8241\n",
            "Epoch 597/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.6116 - accuracy: 0.8308\n",
            "Epoch 598/600\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.5913 - accuracy: 0.8373\n",
            "Epoch 599/600\n",
            "33/33 [==============================] - 2s 56ms/step - loss: 0.5733 - accuracy: 0.8438\n",
            "Epoch 600/600\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.5537 - accuracy: 0.8504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlHndsJTty4Z"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        \n",
        "        #create a mask to prevent [UNK] from bein generated\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:,None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "        #Put a -inf at each bad index\n",
        "        values=[-float(\"inf\")]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        #match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "        \n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        #convert strings to token IDs\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "        \n",
        "        #run the model\n",
        "        #predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        \n",
        "        #use only the last prediction\n",
        "        predicted_logits = predicted_logits [:, -1, :]\n",
        "        predicted_logits = predicted_logits/self.temperature\n",
        "        #apply the prediction mask, prevent \"[UNK]\" from bein generated\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "        \n",
        "        #sample the output logits to generate token IDs\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=1)\n",
        "        \n",
        "        #convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "        \n",
        "        #return the characthers and model state\n",
        "        return predicted_chars, states\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6U5yomqty4a"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2dT_5MJty4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7b5877-a133-4938-f14e-9bf975f30fd1"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"3. Vanity\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "    \n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\"*80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3. Vanity\r\n",
            "Ain't be our hidowa no wrush to foose show Agay not with the Mort\r\n",
            "\r\n",
            "When I wantion lade\r\n",
            "\r\n",
            "[Jamal Made. ArYost of coming\r\n",
            "My songremoking to me?\r\n",
            "Respect the weaking\r\n",
            "an out time\r\n",
            "\r\n",
            "So someoncen to me\r\n",
            "I know and shattered to my jedge.\r\n",
            "What is your name\r\n",
            "Your cangight until that's reading\r\n",
            "you. You have burn lives my lorg on he'll lies are conflew if you.\r\n",
            "Hatce its there, yeah.\r\n",
            "\r\n",
            "Cause to thom for nears\r\n",
            "Love me that I'm time\r\n",
            "\r\n",
            "Felling the ternen before\r\n",
            "You have to some meture\r\n",
            "\r\n",
            "I’'s do begrage the thostend's warke makes\r\n",
            "esce seoning when I say, you'll never beon a will be heard\r\n",
            "Thun times you. Do It Ride taken their start\r\n",
            "Fall\r\n",
            "\r\n",
            "Follow me that's readinate\r\n",
            "i' lose\r\n",
            "Can't here I disfing?\r\n",
            "\r\n",
            "Buit does, I've home\r\n",
            "those off there\r\n",
            "Your burness and hight better that been in privare,\r\n",
            "to swose own my a frarituce it means\r\n",
            "You have to the wrath arrock\r\n",
            "See, dour that I murt now a man\r\n",
            "Make it right\r\n",
            "Trustion the one much more I take?\r\n",
            "\r\n",
            "Mone, punks enough to keep unhthis world  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.991373300552368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkvPNIlPty4b"
      },
      "source": [
        "#### Export the generator\n",
        "This single-step model can  be saved and restored, allowing you to use it anywhere a tf.saved_model is accepted.\n",
        "Run these cells when needed only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOVoxVRJty4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3664513a-9065-4f5e-e1f8-eb6187883806"
      },
      "source": [
        "'''\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntf.saved_model.save(one_step_model, 'one_step')\\none_step_reloaded = tf.saved_model.load('one_step')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxDDx6HUty4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "390e7e49-d9d7-4ecb-e34c-e87abbea00cc"
      },
      "source": [
        "'''\n",
        "states = None\n",
        "next_char = tf.constant([\"Digital\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "    \n",
        "print (tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nstates = None\\nnext_char = tf.constant([\"Digital\"])\\nresult = [next_char]\\n\\nfor n in range(100):\\n    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\\n    result.append(next_char)\\n    \\nprint (tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDRUETFuty4c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}